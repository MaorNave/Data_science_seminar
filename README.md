# Data_science_seminar
Description of a comparison between DNN for segmentation project - data science seminar course

# Abstract
This study provides a thorough review of digital image processing and computer vision.
It includes physical explanations for image acquisition, methodologies for processing and
presenting basic images, algorithmic applications used to address challenges in these fields.
Additionally, it explores business applications and offers a detailed overview of deep learning
models architectures designed for solving segmentation problems. This study investigates
the performance of VGG16 and VGG19 architectures for semantic segmentation tasks using
aerial imagery, focusing on metrics such as accuracy and loss. Through rigorous experi-
mentation, including training with various learning rates, VGG19 demonstrated superior
performance over VGG16, achieving a test accuracy of 83.01% compared to 82.51%. The
VGG19 model’s additional convolutional layers contributed to its enhanced ability to handle
complex segmentation tasks, resulting in more precise and detailed visual predictions.

# Methodology and Implementation
The methodology and implementation of the experiment represented in this research involve
several critical steps, beginning with the use of the ”Semantic Segmentation of Aerial Im-
agery” dataset, sourced from the Kaggle platform. This dataset contains aerial images of
Dubai, captured by MBRSC satellites, and annotated for pixel-wise semantic segmentation
across six classes. With a total of 72 images organized into six larger tiles, the dataset pro-
vides a robust foundation for the experiment. Data preprocessing, including the development
of custom data generator modules to effectively turns the input data to augmented big data
images, which is a key step to ensure the models can efficiently process and learn from the
images. Following data preparation, the methodology covers the fine-tuning of models from
external libraries in Python, specifically focusing on programming a new head (classifier) for
the VGG16 and VGG19 architectures using torchvision models. Both of models architectures
have been trained, validate and tested generically on the same data, with the same number
of epochs and on the same hardware using Google Colab (A100 GPU with 40GB memory)
Finally, the results and metrics presentation will provide a comprehensive evaluation of the
model performances, highlighting the effectiveness of the implemented techniques.
Data Preprocess
In this research, the development of data preprocessing modules was essential for preparing
the model to perform segmentation on aerial images. Using the ”Semantic Segmentation of
Aerial Images” dataset from Kaggle, which features 72 annotated images of Dubai captured
by MBRSC satellites, the data was organized into six larger tiles. To ensure consistency
across all input images, a series of augmentation processes were applied, generating generic
and relevant data for the model. This led to the creation of a custom module named Data
Generator, which handled multiple tasks: cropping images to a standard resolution, sav-
ing relevant images and masks, extracting and saving patches (sized 224×224), performing
augmentations like rotations and flips (8 per each image and mask), and organizing the
processed data into folders for training, validation, and testing. Additionally, the Data
Generator module converted masks into one-hot encoded vectors, simplifying the model’s
architecture during the training phase. The module also redistributed a percentage of data
across these folders to maintain diversity in the training set, ultimately resulting in 4622
test images and masks, 16776 training images, and 5444 validation images. This preprocess-
ing pipeline was crucial for ensuring that the model could generalize well across different
tiles and image resolutions, providing a robust foundation for the segmentation task. By
systematically managing data distribution and format conversion, the module enhanced the
model’s ability to handle variations within the dataset, leading to more accurate segmenta-
tion results.
Models Architectures Fine Tune and Predictions
To effectively train and fine-tune model architectures for the segmentation task, the research
represents implementation process called fine-tuning. This involves adapting existing model
weights using new data generated by the Data Generator module (from last section). To
ensure a uniform classifier across models and isolate differences based on architecture, a
SegmentationModel module was developed. This module allows for the selection of pre-
trained or untrained weights for each model and integrates a new classifier (net head) that
performs an upsampling process. This process involves convolution and factorization of data
with intermediate ReLU layers (5 additional layers in total), resulting in tensor-type data
structures divided into segments representing class probabilities for segmentation (higher
probability in a pixel in a specific dimension number will have a better confidence level to
be the relevant segmentation class). With that, in this research two additional modules
were programmed: TrainValModels and PredictModels. The TrainValModels module offers
functionality for generic training and validation phases of VGG16 and VGG19 architectures,
utilizing TensorBoard to extract metrics and statistics (Loss and Accuracy). Training was
performed across various learning rates (0.00003, 0.0001, 0.0003, 0.001, 0.003) and batch
sizes (64 for training and 32 for validation), with each session conducted for 10 epochs. This
module tracks accuracy and loss metrics, saving the best weights for each model. Furtermore,
TrainValModels module performs a test session on test data by batching 16 images and masks
per each batch iteration resulting each model best params accuracy and loss values. The
PredictModels module, on the other hand, loads the most relevant weights for each model
architecture, bypassing existing weights as needed, and performs predictions on data from
the test set. This ensures that predictions are made with the optimal model weights, based
on data prepared from the test folder.
